{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10621026,"sourceType":"datasetVersion","datasetId":6576210},{"sourceId":246804,"sourceType":"modelInstanceVersion","modelInstanceId":210917,"modelId":232613}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Zindi CGIAR Root Volume Challenge\n\nAs depicted in the picture below, a single cassava plant can have more than one root. The term root volume therefore refers to the total volume of the identified roots of a given plant. In this image, there are at least six roots (labeled from 1 to 6) on the plant.\n\n![](https://zindi-public-release.s3.eu-west-2.amazonaws.com/uploads/image_attachment/image/2559/358c9a4b-527c-4fff-8ff1-725e89510cfd.jpg)\n\nEach folder contains images for the roots of cassava plants generated for the left and right scans at different depths or layers. A Ground Penetrating Radar is used to non-invasively scan the roots of the cassava plants from each side. The raw data collected by the radar is then processed into images. Seven plants are scanned from both sides (left & right) at any given time. This implies that the maximum number of plants in any given image is 7. However this does not mean that all the 7 plants will always be visible in the image of the corresponding scan. The naming convention for images follows this format: XXXXXXXX_S_NNN.png. The characters in the pattern XXXXXXXX consist of random characters. This information is not relevant to you for this project. The S is the side of the scan. It can be L (left) or R (right) depending on the side from which the scan is performed. The three last characters in the image name are digits. These digits correspond to the depth or layer of the scan. Examples of images found in a folder are XXXXXX_L_001.png, XXXXXXXX_R_001.png, XXXXXXX_L_102.png, and XXXXXXXX_R_102.png. In this instance, there are 204 images in the folder; 102 images for each side with layers ranging from 1 up to 102. No assumption should be made about each folder having the same number of images since a different range of layers may be used.\n\n**More about the Competition [here](https://zindi.africa/competitions/cgiar-root-volume-estimation-challenge)**\n\n### PS If you find this notebook helpful, please consider giving it an upvote. Otherwise let's dive into the more interesting part","metadata":{}},{"cell_type":"markdown","source":"# üå± Understanding the Competition üöÄ  \n\nThis competition presents a **unique challenge**, and I truly appreciate the effort the organizers have put into designing it! üéØ Before jumping into coding, let‚Äôs take a step back and **understand the task**.  \n\n## üîç What is Ground Penetrating Radar (GPR)?  \n\nThe dataset in this competition was collected using **Ground Penetrating Radar (GPR)**. Never heard of GPR before? ü§î Don't worry, I was also confused at first! So, I checked out a few YouTube videos, and this one does a great job explaining how it works:  \n\n[![üé• Video Thumbnail](https://img.youtube.com/vi/VIDEO_ID/0.jpg)](https://www.youtube.com/watch?v=MfpUA05s0GU)  \n\nAfter watching the video, the **images in the dataset start to make a lot more sense**! üéâ  \n\n---\n\n## üèÜ Understanding the Task  \n\nEach folder in the dataset contains **GPR-scanned images** of cassava plant roots, captured from the **left and right** sides at different depths. üåø The scans are processed from raw radar data, allowing us to **analyze root structures without digging up the plants**. üõ†Ô∏è  \n\nAt most, an image may contain **up to 7 plants**, though not all may be visible in every scan. üëÄ  \n\n### üéØ Our mission:  \n\n1Ô∏è‚É£ **Segment the root data** ‚Äì Identify and extract root structures from the GPR images.  \n2Ô∏è‚É£ **Estimate the root volume** ‚Äì Use the segmented roots to predict the **total root volume** of each cassava plant.  \n\nSince this is a **starter notebook**, we will begin by using the **pre-trained segmentation models** provided by the organizers. These models are:  \n\n‚úÖ `best_early.pt` ‚Äì Trained on **early-stage** cassava root data.  \n‚úÖ `best_late.pt` ‚Äì Trained on **late-stage** cassava root data.  \n‚úÖ `best_full.pt` ‚Äì Trained on data from **both growth stages**.  \n\nYou can **use these models**, **fine-tune them**, or **develop your own** from scratch! üí° The choice is yours!  \n\n---\n\n## üìÇ Dataset Insights  \n\nüîπ The images follow the naming pattern: **`XXXXXXXX_S_NNN.png`**, where:  \n   - `XXXXXXXX` ‚ûù Random identifier (not relevant to the task).  \n   - `S` ‚ûù Scan **side** (`L` = Left, `R` = Right).  \n   - `NNN` ‚ûù **Depth** layer (e.g., `001`, `052`, `102`).  \n\nüîπ **Left and right images** do not always have the same dimensions. üìè  \nüîπ **Root visibility varies** at different depths, so choosing the right images is **critical**!  \nüîπ A **CSV file** provides suggested image ranges, but you're free to experiment.\n\n---\n\n## üöÄ Next Steps  \n\nNow that we understand the task, let‚Äôs **load the data** and run the provided models to get our **first results**! üéâ  \n\nüîπ **Step 1:** Load the dataset   \nüîπ **Step 2:** Run the segmentation models <br>\nüîπ **Step 3:** Generate a new dataset <br>\nüîπ **Step 4:** Train a new Model on this generated dataset <br>\nüîπ **Step 5:** Run Inference, submit\n\nLet‚Äôs dive in! üî•üöÄ  \n","metadata":{}},{"cell_type":"markdown","source":"# Import the Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom pathlib import Path\n!pip install ultralytics -q\nfrom ultralytics import YOLO\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom typing import List\nimport re\nfrom tqdm import tqdm\n!pip install lightning -q\nimport lightning as L\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nfrom torchvision.transforms import v2\nfrom torch import nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:32:40.971865Z","iopub.execute_input":"2025-02-01T20:32:40.972151Z","iopub.status.idle":"2025-02-01T20:32:47.858155Z","shell.execute_reply.started":"2025-02-01T20:32:40.972129Z","shell.execute_reply":"2025-02-01T20:32:47.85716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the data","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA_PATH = Path(\"/kaggle/input/zindi-cgiar-root-volume-estimation-challenge/data/train\")\nTEST_DATA_PATH = Path(\"/kaggle/input/zindi-cgiar-root-volume-estimation-challenge/data/test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:22.627264Z","iopub.execute_input":"2025-02-01T18:59:22.627607Z","iopub.status.idle":"2025-02-01T18:59:22.63136Z","shell.execute_reply.started":"2025-02-01T18:59:22.627587Z","shell.execute_reply":"2025-02-01T18:59:22.630494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/zindi-cgiar-root-volume-estimation-challenge/Train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/zindi-cgiar-root-volume-estimation-challenge/Test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/zindi-cgiar-root-volume-estimation-challenge/Sample_Submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:22.632849Z","iopub.execute_input":"2025-02-01T18:59:22.633146Z","iopub.status.idle":"2025-02-01T18:59:22.677498Z","shell.execute_reply.started":"2025-02-01T18:59:22.633115Z","shell.execute_reply":"2025-02-01T18:59:22.676742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:24.908131Z","iopub.execute_input":"2025-02-01T18:59:24.908594Z","iopub.status.idle":"2025-02-01T18:59:24.931785Z","shell.execute_reply.started":"2025-02-01T18:59:24.908497Z","shell.execute_reply":"2025-02-01T18:59:24.930964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-01T18:58:37.529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape, train_df.shape, submission_df.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-01T18:58:37.529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-01T18:58:37.529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Stage'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:27.947568Z","iopub.execute_input":"2025-02-01T18:59:27.947907Z","iopub.status.idle":"2025-02-01T18:59:27.957967Z","shell.execute_reply.started":"2025-02-01T18:59:27.947881Z","shell.execute_reply":"2025-02-01T18:59:27.9571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['Stage'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:29.94782Z","iopub.execute_input":"2025-02-01T18:59:29.948221Z","iopub.status.idle":"2025-02-01T18:59:29.954831Z","shell.execute_reply.started":"2025-02-01T18:59:29.948187Z","shell.execute_reply":"2025-02-01T18:59:29.954005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['FolderName'].unique().size, test_df['FolderName'].unique().size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:32.259666Z","iopub.execute_input":"2025-02-01T18:59:32.260042Z","iopub.status.idle":"2025-02-01T18:59:32.266558Z","shell.execute_reply.started":"2025-02-01T18:59:32.260015Z","shell.execute_reply":"2025-02-01T18:59:32.265948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder = \"Ypktwvqjbn\"\ndf = train_df[train_df['FolderName'] == folder]\n\ndf[['FolderName', 'PlantNumber', 'Side', 'Start', 'End', 'RootVolume']].head(n = 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:33.87543Z","iopub.execute_input":"2025-02-01T18:59:33.875751Z","iopub.status.idle":"2025-02-01T18:59:33.89296Z","shell.execute_reply.started":"2025-02-01T18:59:33.875725Z","shell.execute_reply":"2025-02-01T18:59:33.891959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(os.listdir(\"/kaggle/input/zindi-cgiar-root-volume-estimation-challenge/data/test/A6dzrkjqvl\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:36.140283Z","iopub.execute_input":"2025-02-01T18:59:36.140734Z","iopub.status.idle":"2025-02-01T18:59:36.155834Z","shell.execute_reply.started":"2025-02-01T18:59:36.140693Z","shell.execute_reply":"2025-02-01T18:59:36.154862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the segmentation models","metadata":{}},{"cell_type":"code","source":"# Since the organizers didn't provide performance metrics, we will be using all the models (For Now)\nsegmentation_models = {\n    \"full\" : YOLO(\"/kaggle/input/cgiar-yolo-models/pytorch/default/1/Models/best_full.pt\"),\n    \"early\" : YOLO(\"/kaggle/input/cgiar-yolo-models/pytorch/default/1/Models/best_early.pt\"),\n    \"late\" : YOLO(\"/kaggle/input/cgiar-yolo-models/pytorch/default/1/Models/best_late.pt\")\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:37.964251Z","iopub.execute_input":"2025-02-01T18:59:37.964693Z","iopub.status.idle":"2025-02-01T18:59:41.847323Z","shell.execute_reply.started":"2025-02-01T18:59:37.964644Z","shell.execute_reply":"2025-02-01T18:59:41.846573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = \"/kaggle/input/zindi-cgiar-root-volume-estimation-challenge/data/train/A2miww5mfx/A2miww5mfx_L_014.png\"\n\nseg_model = segmentation_models['early']\nresults = seg_model(sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:41.848541Z","iopub.execute_input":"2025-02-01T18:59:41.848866Z","iopub.status.idle":"2025-02-01T18:59:45.95697Z","shell.execute_reply.started":"2025-02-01T18:59:41.848832Z","shell.execute_reply":"2025-02-01T18:59:45.956104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image = Image.open(sample)\nsample_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:45.958301Z","iopub.execute_input":"2025-02-01T18:59:45.958549Z","iopub.status.idle":"2025-02-01T18:59:45.999889Z","shell.execute_reply.started":"2025-02-01T18:59:45.958518Z","shell.execute_reply":"2025-02-01T18:59:45.999132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image.mode","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-01T18:58:37.53Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = results[0].plot()\nplt.imshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:53.947952Z","iopub.execute_input":"2025-02-01T18:59:53.948239Z","iopub.status.idle":"2025-02-01T18:59:54.11942Z","shell.execute_reply.started":"2025-02-01T18:59:53.948217Z","shell.execute_reply":"2025-02-01T18:59:54.118738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for result in results:\n    for box in result.boxes.xywh:\n        x, y, w, h = box  # x, y are center coordinates\n        print(f\"Bounding Box - X: {x}, Y: {y}, Width: {w}, Height: {h}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:56.283915Z","iopub.execute_input":"2025-02-01T18:59:56.284329Z","iopub.status.idle":"2025-02-01T18:59:56.291914Z","shell.execute_reply.started":"2025-02-01T18:59:56.284293Z","shell.execute_reply":"2025-02-01T18:59:56.291065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"segmentation_models.keys()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-01T18:58:37.53Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef get_segmented_images(image_paths, display_image=False):\n    \"\"\"Extracts and merges segments from images, returning only images with detections.\"\"\"\n\n    for model in segmentation_models.keys():\n        model = segmentation_models[model]\n        results = model(image_paths, verbose=False)\n\n        if len(results[0].boxes.xyxy) != 0:\n            break\n\n    if len(results[0].boxes.xyxy) == 0:\n        # Incase of no detections, return all the images (Still thinking of better ways to overcome this)\n        return [Image.open(img) for img in image_paths]\n        \n    segmented_images = []\n    \n    for img_path, result in zip(image_paths, results):\n        original_image = Image.open(img_path)\n        merged_image = Image.new(\"RGBA\", original_image.size, (0, 0, 0, 0))\n        \n        # Skip if no detections\n        if len(result.boxes.xyxy) == 0:\n            continue \n            \n        # Extract and paste segments\n        for box in result.boxes.xyxy:\n            x1, y1, x2, y2 = map(int, box.tolist())\n            segment = original_image.crop((x1, y1, x2, y2))\n            merged_image.paste(segment, (x1, y1))\n            \n        segmented_images.append(merged_image)\n\n    # Display Images\n    if display_image and segmented_images:\n        fig, axes = plt.subplots(1, len(segmented_images), figsize=(15, 10))\n        if len(segmented_images) == 1:\n            axes = [axes]\n        for ax, img in zip(axes, segmented_images):\n            ax.imshow(img)\n            ax.axis(\"off\")\n        plt.show()\n\n    return segmented_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:59:59.803932Z","iopub.execute_input":"2025-02-01T18:59:59.804225Z","iopub.status.idle":"2025-02-01T18:59:59.81133Z","shell.execute_reply.started":"2025-02-01T18:59:59.804202Z","shell.execute_reply":"2025-02-01T18:59:59.810414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation** <br>\nWe are going to use `get_segmented_images` function to process a set of images and extracts the regions of interest (i.e., root segments) using a pre-trained segmentation models. It then returns the segmented images where roots are detected, or all images if no detections are made.","metadata":{}},{"cell_type":"code","source":"get_segmented_images([sample], display_image = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:00:02.278365Z","iopub.execute_input":"2025-02-01T19:00:02.278709Z","iopub.status.idle":"2025-02-01T19:00:03.078411Z","shell.execute_reply.started":"2025-02-01T19:00:02.278661Z","shell.execute_reply":"2025-02-01T19:00:03.077741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The function retrieves images from a specified folder\n# based on the scan side (left or right) and a layer range\n# (from a starting layer to an ending layer).\n\ndef get_images_within_range(base_path: Path, folder: str, side: str, start: int, end: int) -> list[Path]:\n    \"\"\"\n    Get images from a folder that match the specified side (L/R) and layer range.\n    \n    Args:\n        base_path: Root directory containing all folders\n        folder: Name of the target folder (e.g., 'Ypktwvqjbn')\n        side: Scan side to filter ('L' or 'R')\n        start: Starting layer (inclusive)\n        end: Ending layer (inclusive)\n    \"\"\"\n    folder_path = base_path / folder\n    \n    # Get all files in the folder\n    try:\n        images = os.listdir(folder_path)\n    except FileNotFoundError:\n        return []\n\n    # Regex pattern to extract side and layer from filenames\n    pattern = re.compile(r'_([LR])_(\\d{3})\\.png$')\n    \n    selected_images = []\n    \n    for img_name in images:\n        match = pattern.search(img_name)\n        if match:\n            # Extract side and layer from filename\n            img_side = match.group(1)\n            layer = int(match.group(2))\n            \n            # Check if matches criteria\n            if img_side == side and start <= layer <= end:\n                selected_images.append(folder_path / img_name)\n    \n    return selected_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:00:04.787798Z","iopub.execute_input":"2025-02-01T19:00:04.788095Z","iopub.status.idle":"2025-02-01T19:00:04.793322Z","shell.execute_reply.started":"2025-02-01T19:00:04.788074Z","shell.execute_reply":"2025-02-01T19:00:04.792453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test and see\nsample_images = get_images_within_range(TRAIN_DATA_PATH, \"Ypktwvqjbn\", \"L\", 33, 41)\nsample_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:00:07.204171Z","iopub.execute_input":"2025-02-01T19:00:07.204486Z","iopub.status.idle":"2025-02-01T19:00:07.217252Z","shell.execute_reply.started":"2025-02-01T19:00:07.204459Z","shell.execute_reply":"2025-02-01T19:00:07.216523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_segmented_images(sample_images, display_image = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:00:10.787989Z","iopub.execute_input":"2025-02-01T19:00:10.78831Z","iopub.status.idle":"2025-02-01T19:00:11.7431Z","shell.execute_reply.started":"2025-02-01T19:00:10.788282Z","shell.execute_reply":"2025-02-01T19:00:11.742284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# finally merge all of the image segments into one\ndef merge_segmented_images(path: Path, folder: str, side: str, start: int, end: int):\n    images_in_range = get_images_within_range(path, folder, side, start, end)\n    segmented_images = get_segmented_images(images_in_range)\n\n    # Determine final dimensions for the merged image\n    total_width = sum(img.width for img in segmented_images)  # Sum of all widths\n    max_height = max(img.height for img in segmented_images)  # Max height among all images\n\n    # Create a blank canvas with a transparent background\n    merged_image = Image.new(\"RGBA\", (total_width, max_height), (0, 0, 0, 0))\n\n    # Paste each segmented image next to the previous one (left to right)\n    x_offset = 0\n    for img in segmented_images:\n        merged_image.paste(img, (x_offset, 0), img)  # Paste at correct position\n        x_offset += img.width  # Move x-offset to the right for the next image\n\n    return merged_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:54:05.486417Z","iopub.execute_input":"2025-02-01T19:54:05.486807Z","iopub.status.idle":"2025-02-01T19:54:05.492853Z","shell.execute_reply.started":"2025-02-01T19:54:05.486777Z","shell.execute_reply":"2025-02-01T19:54:05.491949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_image = merge_segmented_images(TRAIN_DATA_PATH, \"Ox18ob0syv\", \"R\", 21, 28)\nmerged_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:54:10.182334Z","iopub.execute_input":"2025-02-01T19:54:10.182668Z","iopub.status.idle":"2025-02-01T19:54:10.282785Z","shell.execute_reply.started":"2025-02-01T19:54:10.18263Z","shell.execute_reply":"2025-02-01T19:54:10.282066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_IMAGE_DIR = Path(\"/kaggle/working/merged_images/\")\nos.makedirs(OUTPUT_IMAGE_DIR, exist_ok = True)\n\nTRAIN_OUTPUT_DIR = OUTPUT_IMAGE_DIR / \"Train\"\nTEST_OUTPUT_DIR = OUTPUT_IMAGE_DIR / \"Test\"\n\nos.makedirs(TRAIN_OUTPUT_DIR, exist_ok = True)\nos.makedirs(TEST_OUTPUT_DIR, exist_ok = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:55:24.366195Z","iopub.execute_input":"2025-02-01T19:55:24.366505Z","iopub.status.idle":"2025-02-01T19:55:24.371266Z","shell.execute_reply.started":"2025-02-01T19:55:24.366482Z","shell.execute_reply":"2025-02-01T19:55:24.370493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:55:27.553174Z","iopub.execute_input":"2025-02-01T19:55:27.553487Z","iopub.status.idle":"2025-02-01T19:55:27.565365Z","shell.execute_reply.started":"2025-02-01T19:55:27.553464Z","shell.execute_reply":"2025-02-01T19:55:27.564668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate the new dataset","metadata":{}},{"cell_type":"code","source":"# Iterate through all the  images in the df\ndef generate_merged_images(df: pd.DataFrame, output_path: Path, input_path: Path):\n    gen_image_paths = []\n    \n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Merging Images\"):\n        gen_image = merge_segmented_images(\n            path=input_path,\n            folder=row[\"FolderName\"],\n            side=row[\"Side\"],\n            start=row[\"Start\"],\n            end=row[\"End\"]\n        )\n        img_path = output_path / f\"{row['ID']}.png\"\n        gen_image.save(img_path)\n        gen_image_paths.append(img_path)\n\n    df['image segments'] = gen_image_paths\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:55:36.493243Z","iopub.execute_input":"2025-02-01T19:55:36.493561Z","iopub.status.idle":"2025-02-01T19:55:36.498603Z","shell.execute_reply.started":"2025-02-01T19:55:36.493537Z","shell.execute_reply":"2025-02-01T19:55:36.497852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_train_df = generate_merged_images(train_df, TRAIN_OUTPUT_DIR, TRAIN_DATA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:55:39.896611Z","iopub.execute_input":"2025-02-01T19:55:39.896972Z","iopub.status.idle":"2025-02-01T19:56:42.83344Z","shell.execute_reply.started":"2025-02-01T19:55:39.896945Z","shell.execute_reply":"2025-02-01T19:56:42.832745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:58:09.688923Z","iopub.execute_input":"2025-02-01T19:58:09.689262Z","iopub.status.idle":"2025-02-01T19:58:09.700554Z","shell.execute_reply.started":"2025-02-01T19:58:09.689238Z","shell.execute_reply":"2025-02-01T19:58:09.699907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_test_df = generate_merged_images(test_df, TEST_OUTPUT_DIR, TEST_DATA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:58:16.721738Z","iopub.execute_input":"2025-02-01T19:58:16.722031Z","iopub.status.idle":"2025-02-01T19:58:37.341654Z","shell.execute_reply.started":"2025-02-01T19:58:16.722008Z","shell.execute_reply":"2025-02-01T19:58:37.340735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_generated_images(image_paths):\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    axes = axes.flatten()\n\n    for i, img_path in enumerate(image_paths[:4]):  # Display first 4 images\n        image = Image.open(img_path)\n        axes[i].imshow(image)\n        axes[i].axis(\"off\")  # Hide axes\n        axes[i].set_title(f\"Image {i+1}\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T19:59:58.704637Z","iopub.execute_input":"2025-02-01T19:59:58.705026Z","iopub.status.idle":"2025-02-01T19:59:58.71017Z","shell.execute_reply.started":"2025-02-01T19:59:58.704997Z","shell.execute_reply":"2025-02-01T19:59:58.709353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display Train\n\ndisplay_generated_images(new_train_df['image segments'].values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:00:03.9463Z","iopub.execute_input":"2025-02-01T20:00:03.946603Z","iopub.status.idle":"2025-02-01T20:00:04.182989Z","shell.execute_reply.started":"2025-02-01T20:00:03.946569Z","shell.execute_reply":"2025-02-01T20:00:04.18216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_generated_images(new_test_df['image segments'].values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:00:19.792938Z","iopub.execute_input":"2025-02-01T20:00:19.793239Z","iopub.status.idle":"2025-02-01T20:00:20.022938Z","shell.execute_reply.started":"2025-02-01T20:00:19.793214Z","shell.execute_reply":"2025-02-01T20:00:20.022144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:00:28.848717Z","iopub.execute_input":"2025-02-01T20:00:28.84905Z","iopub.status.idle":"2025-02-01T20:00:28.856715Z","shell.execute_reply.started":"2025-02-01T20:00:28.849026Z","shell.execute_reply":"2025-02-01T20:00:28.855963Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Traning","metadata":{}},{"cell_type":"code","source":"train_transform = v2.Compose([\n    v2.Resize(size=(20, 150), antialias=True),\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),  \n    v2.Normalize(mean=[0.5], std=[0.5])\n])\n\n# Testing Transform\ntest_transform = v2.Compose([\n    v2.Resize(size=(20, 150), antialias=True),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),  \n    v2.Normalize(mean=[0.5], std=[0.5])\n])\n\nimg = train_transform(sample_image)\nimg.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:48:53.96999Z","iopub.execute_input":"2025-02-01T20:48:53.970302Z","iopub.status.idle":"2025-02-01T20:48:53.979863Z","shell.execute_reply.started":"2025-02-01T20:48:53.970278Z","shell.execute_reply":"2025-02-01T20:48:53.979095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img.size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image = Image.open(new_train_df['image segments'].iloc[6])\ntrain_transform(sample_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:07:06.909506Z","iopub.execute_input":"2025-02-01T20:07:06.909837Z","iopub.status.idle":"2025-02-01T20:07:06.918568Z","shell.execute_reply.started":"2025-02-01T20:07:06.909803Z","shell.execute_reply":"2025-02-01T20:07:06.917637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define the dataset\nclass RootVolumeDataset(Dataset):\n    def __init__(self, df : pd.DataFrame, transform = None, is_train = True):\n        super().__init__()\n        self.df = df\n        self.transform = transform\n        self.is_train = is_train\n\n    def __getitem__(self, index):\n        image = Image.open(self.df['image segments'].iloc[index]).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        if self.is_train:\n            label = self.df['RootVolume'].iloc[index]\n\n            return image, torch.tensor(label, dtype=torch.float32)\n\n        return image\n\n    def __len__(self):\n        return len(self.df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:58:20.550657Z","iopub.execute_input":"2025-02-01T20:58:20.551018Z","iopub.status.idle":"2025-02-01T20:58:20.556796Z","shell.execute_reply.started":"2025-02-01T20:58:20.550993Z","shell.execute_reply":"2025-02-01T20:58:20.555765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed):\n    torch.manual_seed(seed) \n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True \n    torch.backends.cudnn.benchmark = False\n    L.pytorch.seed_everything(seed, workers=True)\n    \nseed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:49:14.361807Z","iopub.execute_input":"2025-02-01T20:49:14.362092Z","iopub.status.idle":"2025-02-01T20:49:14.369743Z","shell.execute_reply.started":"2025-02-01T20:49:14.362071Z","shell.execute_reply":"2025-02-01T20:49:14.368789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = RootVolumeDataset(new_train_df, train_transform)\ntest_dataset = RootVolumeDataset(new_test_df, test_transform, is_train = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:58:25.300976Z","iopub.execute_input":"2025-02-01T20:58:25.301265Z","iopub.status.idle":"2025-02-01T20:58:25.305077Z","shell.execute_reply.started":"2025-02-01T20:58:25.301243Z","shell.execute_reply":"2025-02-01T20:58:25.304144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0][0].shape, train_dataset[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:58:51.469029Z","iopub.execute_input":"2025-02-01T20:58:51.46932Z","iopub.status.idle":"2025-02-01T20:58:51.483739Z","shell.execute_reply.started":"2025-02-01T20:58:51.469298Z","shell.execute_reply":"2025-02-01T20:58:51.483005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=0,\n    pin_memory=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:59:05.679873Z","iopub.execute_input":"2025-02-01T20:59:05.680165Z","iopub.status.idle":"2025-02-01T20:59:05.684455Z","shell.execute_reply.started":"2025-02-01T20:59:05.680143Z","shell.execute_reply":"2025-02-01T20:59:05.683608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# simple CNN for the predictions\nclass RootVolumeRegressor(L.LightningModule):\n    def __init__(self, lr=1e-3):\n        super().__init__()\n        self.lr = lr\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        # Fully Connected Regression Head\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)  # Regression output\n        )\n\n        self.criterion = nn.MSELoss()\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        images, targets = batch\n        preds = self(images).squeeze()\n        loss = self.criterion(preds, targets)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, targets = batch\n        preds = self(images).squeeze()\n        loss = self.criterion(preds, targets)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\nmodel = RootVolumeRegressor()\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:59:10.109707Z","iopub.execute_input":"2025-02-01T20:59:10.11011Z","iopub.status.idle":"2025-02-01T20:59:10.122882Z","shell.execute_reply.started":"2025-02-01T20:59:10.110073Z","shell.execute_reply":"2025-02-01T20:59:10.122194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training using torch lightning ‚ö°","metadata":{}},{"cell_type":"code","source":"# \ntrainer = L.Trainer(max_epochs = 20)\n\ntrainer.fit(model, train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:59:16.638805Z","iopub.execute_input":"2025-02-01T20:59:16.639132Z","iopub.status.idle":"2025-02-01T20:59:47.014913Z","shell.execute_reply.started":"2025-02-01T20:59:16.639104Z","shell.execute_reply":"2025-02-01T20:59:47.014059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model_preds(model, dataloader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    preds, targets = [], []\n    model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)  # Predict\n            preds.extend(outputs.cpu().numpy().flatten())\n            targets.extend(labels.cpu().numpy().flatten())\n\n    return np.array(preds), np.array(targets)\n\ntrain_preds, target = get_model_preds(model, train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:10:10.19488Z","iopub.execute_input":"2025-02-01T21:10:10.195245Z","iopub.status.idle":"2025-02-01T21:10:11.495075Z","shell.execute_reply.started":"2025-02-01T21:10:10.195215Z","shell.execute_reply":"2025-02-01T21:10:11.494135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_rmse(preds, targets):\n    \"\"\"\n    Compute Root Mean Squared Error (RMSE) between predictions and ground truth targets.\n    \"\"\"\n    preds = np.array(preds) if not isinstance(preds, np.ndarray) else preds\n    targets = np.array(targets) if not isinstance(targets, np.ndarray) else targets\n    \n    return np.sqrt(np.mean((preds - targets) ** 2))\n\ncalculate_rmse(train_preds, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:10:20.451969Z","iopub.execute_input":"2025-02-01T21:10:20.452291Z","iopub.status.idle":"2025-02-01T21:10:20.459022Z","shell.execute_reply.started":"2025-02-01T21:10:20.452263Z","shell.execute_reply":"2025-02-01T21:10:20.458131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_test_preds(model, dataloader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    preds = []\n    model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            images = batch if isinstance(batch, torch.Tensor) else batch[0]\n            images = images.to(device)\n\n            outputs = model(images)\n            preds.extend(outputs.cpu().numpy().flatten()) \n\n    return np.array(preds)\n\ntest_preds = get_test_preds(model, test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:12:50.547296Z","iopub.execute_input":"2025-02-01T21:12:50.547637Z","iopub.status.idle":"2025-02-01T21:12:50.96107Z","shell.execute_reply.started":"2025-02-01T21:12:50.547612Z","shell.execute_reply":"2025-02-01T21:12:50.960342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['RootVolume'] = test_preds\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:13:37.258122Z","iopub.execute_input":"2025-02-01T21:13:37.258449Z","iopub.status.idle":"2025-02-01T21:13:37.271984Z","shell.execute_reply.started":"2025-02-01T21:13:37.258419Z","shell.execute_reply":"2025-02-01T21:13:37.271089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = test_df[['ID', 'RootVolume']]\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:15:04.369201Z","iopub.execute_input":"2025-02-01T21:15:04.369506Z","iopub.status.idle":"2025-02-01T21:15:04.380344Z","shell.execute_reply.started":"2025-02-01T21:15:04.369484Z","shell.execute_reply":"2025-02-01T21:15:04.379666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üéâ Conclusion\n\nThank you so much for reading through this notebook! I hope it has provided valuable insights and a solid starting point for the **root volume prediction** task. üå± This is just the beginning, and I trust that it will help you make progress in building better predictions for cassava root volumes. üí°\n\nFeel free to share any comments, suggestions, or improvements‚ÄîI'd love to hear your thoughts!\n\n## ‚ú® PS:  \n### If you found this notebook helpful, please consider giving it an upvote. Your feedback is greatly appreciated! üëç Happy coding, and good luck! üöÄ\n","metadata":{}}]}