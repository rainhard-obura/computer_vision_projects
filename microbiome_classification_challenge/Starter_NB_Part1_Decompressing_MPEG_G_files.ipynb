{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f757ef79-0473-4c17-a9f8-79c20633dc87",
      "metadata": {
        "id": "f757ef79-0473-4c17-a9f8-79c20633dc87"
      },
      "source": [
        "# 🧠 Part 1: The Plan for unzipping the Train and Test folders and view the MPEG-G files\n",
        "We’ll create a local Jupyter notebook or script that:\n",
        "\n",
        "0. 🔧 Step 0: Prerequisites\n",
        "1. 🐳 Pulls and runs the Genie Docker container\n",
        "2. 📤 Extract the Train and Test File from TrainFiles.zip and TestFiles.zip\n",
        "3. 🐳 Pull the Genie Docker Image\n",
        "4. 📂 Mounts your ```.mgb``` file for one file in TrainFiles.\n",
        "5. 🧬 Load FASTQ in Python\n",
        "6. 💡 Explain the layers and outputs in plain English for a data scientist unfamiliar with bioinformatics\n",
        "7. 🏃 Run steps 4 & 5 for all files in TrainFiles and TestFiles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6c544b-60de-4a99-b562-1801718423b4",
      "metadata": {
        "id": "4f6c544b-60de-4a99-b562-1801718423b4"
      },
      "source": [
        "## 📓 Let’s Start with a Local Notebook\n",
        "Here's how your local notebook should look:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1c3f32-5008-443a-92ef-cda803562cb5",
      "metadata": {
        "id": "ba1c3f32-5008-443a-92ef-cda803562cb5",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### 🔧 Step 0: Prerequisites\n",
        "\n",
        "1. Install an Integrated Development environment VSCode or Anaconda\n",
        "2. Install Docker (if you haven’t already):\n",
        "👉 https://www.docker.com/products/docker-desktop/\n",
        "3. Install Python\n",
        "4. Install Conda\n",
        "5. Create a virtual invironment\n",
        "\n",
        "Note: doing this you may be asked to install command line tools depending on your machine, xcode, git, etc, but it is worth it as you will find them useful. Some of the downloads may take more than half an hour, stay focused and follow instructions.\n",
        "\n",
        " Instructions for Mac are available [here](https://docs.google.com/document/d/1Rug61nxs6FLqlYh8Qzqgzuv6nq3lyTq45HVB6cqERfE/edit?usp=sharing)  \n",
        "If you already have IDE, Python, CondaInstall Docker:\n",
        "👉 https://www.docker.com/products/docker-desktop/\n",
        "\n",
        "Important:\n",
        "\n",
        " However, we recommend that you install Biopython using conda, as it will also install the required dependencies.\n",
        "\n",
        "` conda install Bio`\n",
        "\n",
        " A better approach is to create a new conda environment for your project, and then install Biopython in that environment:\n",
        "\n",
        " `conda create -n myenv mpegg_env `\n",
        " Activate the environment\n",
        "\n",
        " `conda activate mpegg_env`\n",
        " Now you can install Biopython in the new environment  and then import Bio in your code\n",
        "\n",
        "` conda install -c conda-forge biopython`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02ad8d83-b7c2-4127-976d-a4830a656970",
      "metadata": {
        "id": "02ad8d83-b7c2-4127-976d-a4830a656970",
        "outputId": "0f2a2e96-bd38-44d9-e79b-8e6eded984c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hello from Docker!\n",
            "This message shows that your installation appears to be working correctly.\n",
            "\n",
            "To generate this message, Docker took the following steps:\n",
            " 1. The Docker client contacted the Docker daemon.\n",
            " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
            "    (amd64)\n",
            " 3. The Docker daemon created a new container from that image which runs the\n",
            "    executable that produces the output you are currently reading.\n",
            " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
            "    to your terminal.\n",
            "\n",
            "To try something more ambitious, you can run an Ubuntu container with:\n",
            " $ docker run -it ubuntu bash\n",
            "\n",
            "Share images, automate workflows, and more with a free Docker ID:\n",
            " https://hub.docker.com/\n",
            "\n",
            "For more examples and ideas, visit:\n",
            " https://docs.docker.com/get-started/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test Docker works\n",
        "!docker run hello-world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2b575244-916f-4e82-8ab4-325dc13848f9",
      "metadata": {
        "id": "2b575244-916f-4e82-8ab4-325dc13848f9",
        "outputId": "ef82b582-8b4e-4da9-e61a-1088d6115ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: biopython>=1.80 in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from Bio) (1.85)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from Bio)\n",
            "  Downloading pandas-2.3.1-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
            "Collecting pooch (from Bio)\n",
            "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests (from Bio)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tqdm (from Bio)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from biopython>=1.80->Bio) (2.2.6)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting httpx>=0.22.0 (from biothings-client>=0.2.6->mygene->Bio)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting anyio (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
            "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting certifi (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting idna (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.3.0)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from pandas->Bio) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->Bio)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->Bio)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.17.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from pooch->Bio) (4.3.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from pooch->Bio) (25.0)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->Bio)\n",
            "  Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->Bio)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\reinhard\\anaconda3\\envs\\mpegg_env\\lib\\site-packages (from tqdm->Bio) (0.4.6)\n",
            "Downloading bio-1.8.0-py3-none-any.whl (321 kB)\n",
            "Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Downloading pandas-2.3.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
            "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 1.0/11.3 MB 6.3 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 1.8/11.3 MB 4.2 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 3.1/11.3 MB 4.4 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 3.9/11.3 MB 4.3 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 4.7/11.3 MB 4.2 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 5.2/11.3 MB 4.1 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 5.8/11.3 MB 3.8 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 6.0/11.3 MB 3.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 6.6/11.3 MB 3.2 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 6.8/11.3 MB 3.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 7.1/11.3 MB 2.8 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 7.3/11.3 MB 2.7 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 7.3/11.3 MB 2.7 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 7.6/11.3 MB 2.6 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 7.9/11.3 MB 2.4 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 8.1/11.3 MB 2.3 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 8.4/11.3 MB 2.3 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 8.7/11.3 MB 2.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 8.9/11.3 MB 2.2 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.2/11.3 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 9.4/11.3 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 9.7/11.3 MB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.0/11.3 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.2/11.3 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.5/11.3 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.7/11.3 MB 1.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.0/11.3 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.3/11.3 MB 1.8 MB/s eta 0:00:00\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: pytz, urllib3, tzdata, tqdm, sniffio, idna, h11, charset_normalizer, certifi, requests, pandas, httpcore, anyio, pooch, httpx, gprofiler-official, biothings-client, mygene, Bio\n",
            "\n",
            "   ----------------------------------------  0/19 [pytz]\n",
            "   ----------------------------------------  0/19 [pytz]\n",
            "   ----------------------------------------  0/19 [pytz]\n",
            "   ----------------------------------------  0/19 [pytz]\n",
            "   ----------------------------------------  0/19 [pytz]\n",
            "   ----------------------------------------  0/19 [pytz]\n",
            "   -- -------------------------------------  1/19 [urllib3]\n",
            "   -- -------------------------------------  1/19 [urllib3]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ---- -----------------------------------  2/19 [tzdata]\n",
            "   ------ ---------------------------------  3/19 [tqdm]\n",
            "   ---------- -----------------------------  5/19 [idna]\n",
            "   ---------- -----------------------------  5/19 [idna]\n",
            "   -------------- -------------------------  7/19 [charset_normalizer]\n",
            "   ------------------ ---------------------  9/19 [requests]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   --------------------- ------------------ 10/19 [pandas]\n",
            "   ----------------------- ---------------- 11/19 [httpcore]\n",
            "   ----------------------- ---------------- 11/19 [httpcore]\n",
            "   ------------------------- -------------- 12/19 [anyio]\n",
            "   ------------------------- -------------- 12/19 [anyio]\n",
            "   --------------------------- ------------ 13/19 [pooch]\n",
            "   ----------------------------- ---------- 14/19 [httpx]\n",
            "   ----------------------------- ---------- 14/19 [httpx]\n",
            "   --------------------------------- ------ 16/19 [biothings-client]\n",
            "   ------------------------------------- -- 18/19 [Bio]\n",
            "   ------------------------------------- -- 18/19 [Bio]\n",
            "   ------------------------------------- -- 18/19 [Bio]\n",
            "   ------------------------------------- -- 18/19 [Bio]\n",
            "   ---------------------------------------- 19/19 [Bio]\n",
            "\n",
            "Successfully installed Bio-1.8.0 anyio-4.10.0 biothings-client-0.4.1 certifi-2025.8.3 charset_normalizer-3.4.2 gprofiler-official-1.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 mygene-3.2.2 pandas-2.3.1 pooch-1.8.2 pytz-2025.2 requests-2.32.4 sniffio-1.3.1 tqdm-4.67.1 tzdata-2025.2 urllib3-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Bio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76e459f-00d9-4562-89b8-96b593b338bf",
      "metadata": {
        "id": "d76e459f-00d9-4562-89b8-96b593b338bf"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#%pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "\n",
        "\n",
        "from Bio import SeqIO\n",
        "from collections import Counter\n",
        "\n",
        "#%pip install scikit-learn\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Errors ignore\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3016962c-4346-4c93-a98b-3a50a93a9fc6",
      "metadata": {
        "id": "3016962c-4346-4c93-a98b-3a50a93a9fc6"
      },
      "source": [
        "## Step 1. 📤 Extract the Train and Test File from a .zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "t589j2tflNVs",
      "metadata": {
        "id": "t589j2tflNVs"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "print(os.listdir('.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0d35c0ec-ca34-4702-bada-64928c1f4805",
      "metadata": {
        "id": "0d35c0ec-ca34-4702-bada-64928c1f4805",
        "outputId": "909045d7-daeb-4091-81d8-8678720d8f13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Extracted TrainFiles.zip to ././/\n",
            "✅ Extracted TestFiles.zip to ././/\n"
          ]
        }
      ],
      "source": [
        "# Define your zip files and corresponding output directories\n",
        "zip_targets = {\n",
        "    'TrainFiles.zip': './',\n",
        "    'TestFiles.zip': './'\n",
        "}\n",
        "\n",
        "for zip_path, extract_to in zip_targets.items():\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    # Extract zip content\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "        print(f\"✅ Extracted {zip_path} to ./{extract_to}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17685824-7367-453c-8927-c42a1be05faa",
      "metadata": {
        "id": "17685824-7367-453c-8927-c42a1be05faa"
      },
      "source": [
        "## 🐳 Step 2: Pull the Genie Docker Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "665031c8-871b-4aff-a65c-a359d82af235",
      "metadata": {
        "id": "665031c8-871b-4aff-a65c-a359d82af235",
        "outputId": "0e6bf544-d52d-4cb5-816b-733f3f9dbde3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "latest: Pulling from muefab/genie\n",
            "Digest: sha256:c3112a3879cc18061bbab5ed8f76dec255ab1be46e2133cd59320dd5ba98ef89\n",
            "Status: Image is up to date for muefab/genie:latest\n",
            "docker.io/muefab/genie:latest\n"
          ]
        }
      ],
      "source": [
        "# Ensure you have the latest version of Genie\n",
        "!docker pull muefab/genie:latest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2419079d",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1929300137.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    <cmd> <dir>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "!docker run --rm -v <dir>:/work muefab/genie:latest\n",
        "<cmd> <dir>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "736a0714-fe8a-4965-8177-8df0556eb4ad",
      "metadata": {
        "id": "736a0714-fe8a-4965-8177-8df0556eb4ad"
      },
      "source": [
        "## 🏃 Steps 4 - 8 for one MPEG-G file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "856fc2c5-98c5-4c49-b1a2-f5a71a64c763",
      "metadata": {
        "id": "856fc2c5-98c5-4c49-b1a2-f5a71a64c763"
      },
      "source": [
        "### 📂 Step 4: Mounts your ```.mgb``` file for one file in TrainFiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "783c14b1-26b0-49a4-ba6d-58cda5fad195",
      "metadata": {
        "id": "783c14b1-26b0-49a4-ba6d-58cda5fad195",
        "outputId": "95bf16ac-38de-46ba-a564-ac0ac2803878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 Host path to `.mgb`: c:\\Users\\Reinhard\\Documents\\computer_vision_projects\\microbiome_classification_challenge\\TestFiles\\ID_ZZWUCJ.mgb\n",
            "📁 Host directory mounted: c:\\Users\\Reinhard\\Documents\\computer_vision_projects\\microbiome_classification_challenge\\TestFiles\n",
            "📦 Container directory will be: /data\n",
            "📄 Output FASTQ: ID_ZZWUCJ.fastq\n"
          ]
        }
      ],
      "source": [
        "notebook_dir = os.getcwd()\n",
        "\n",
        "# Pick one `.mgb` file from TrainFiles\n",
        "mgb_filename = \"ID_ZZWUCJ.mgb\"\n",
        "mgb_filename_no_mgb = mgb_filename[:-4]\n",
        "train_dir = os.path.join(os.getcwd(), \"TestFiles\")\n",
        "mgb_file_path = os.path.join(train_dir, mgb_filename)\n",
        "\n",
        "# Output location for decoded FASTQ\n",
        "output_fastq = f\"{mgb_filename_no_mgb}.fastq\"\n",
        "\n",
        "# Docker mount paths\n",
        "host_dir = train_dir                # Local directory with the `.mgb` file\n",
        "container_dir = \"/data\"             # Directory inside the container\n",
        "\n",
        "# Show paths\n",
        "print(f\"📁 Host path to `.mgb`: {mgb_file_path}\")\n",
        "print(f\"📁 Host directory mounted: {host_dir}\")\n",
        "print(f\"📦 Container directory will be: {container_dir}\")\n",
        "print(f\"📄 Output FASTQ: {output_fastq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "409931dd-d615-456f-bef6-35e8a593b754",
      "metadata": {
        "id": "409931dd-d615-456f-bef6-35e8a593b754"
      },
      "source": [
        "### 🔍 Step 5: Decodes ```.mgb``` to .```fastq``` of that one file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ac77f66a-a79d-4725-ae94-78c71d0f43b0",
      "metadata": {
        "id": "ac77f66a-a79d-4725-ae94-78c71d0f43b0",
        "outputId": "b8ead1aa-bd4a-49e1-aefb-28ee258d9e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running: docker run --rm -v .:/work muefab/genie:latest run -f -i /work/TestFiles/ID_ZZWUCJ.mgb -o /work/TestFiles/ID_ZZWUCJ.fastq\n",
            "\n",
            "--- STDOUT ---\n",
            "\n",
            "[INFO,      0.000s, App]:    ______           _\n",
            "[INFO,      0.001s, App]:   / ____/__  ____  (_)__\n",
            "[INFO,      0.001s, App]:  / / __/ _ \\/ __ \\/ / _ \\\n",
            "[INFO,      0.001s, App]: / /_/ /  __/ / / / /  __/\n",
            "[INFO,      0.001s, App]: \\____/\\___/_/ /_/_/\\___/\n",
            "[INFO,      0.001s, App]: Command: /usr/local/bin/genie run -f -i /work/TestFiles/ID_ZZWUCJ.mgb -o /work/TestFiles/ID_ZZWUCJ.fastq \n",
            "[INFO,      0.026s, App/Run]: Input file 1: /work/TestFiles/ID_ZZWUCJ.mgb with size 5.43MiB\n",
            "[INFO,      0.063s, App/Run]: Working directory: /work/TestFiles with 11.7GiB available\n",
            "[INFO,      0.095s, App/Run]: Output file: /work/TestFiles/ID_ZZWUCJ.fastq with 11.7GiB available\n",
            "[INFO,      0.095s, App/Run]: Threads: 4 with 4 supported\n",
            "[INFO,      0.106s, Spring]: Temporary directory: /work/TestFiles/tmp.x7KUBpRZFm/\n",
            "[INFO,      0.116s, Spring]: Temporary directory: /work/TestFiles/tmp.5Zl3P6iWYp/\n",
            "[INFO,      0.135s, Mgb]: Found PS 0...\n",
            "[INFO,      0.358s, Mgb]: AU 0: class U, 163440 records (Global Assembly)\n",
            "[INFO,      0.359s, Mgb]: Progress: 5% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 10% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 15% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 20% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 25% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 30% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 35% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 40% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 45% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 50% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 55% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 60% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 65% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 70% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 75% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 80% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 85% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 90% of file read\n",
            "[INFO,      0.359s, Mgb]: Progress: 95% of file read\n",
            "[INFO,     24.856s, Stats]: size-fastq-name                         sum: 10657658        \n",
            "[INFO,     24.856s, Stats]: size-fastq-quality                      sum: 20348280        \n",
            "[INFO,     24.856s, Stats]: size-fastq-sequence                     sum: 20348280        \n",
            "[INFO,     24.856s, Stats]: size-fastq-total                        sum: 51354218        \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmpos-position-comp           sum: 47434           \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmpos-position-raw            sum: 678080          \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmpos-terminator-comp         sum: 15779           \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmpos-terminator-raw          sum: 103089          \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmtype-substitution-comp      sum: 12408           \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmtype-substitution-raw       sum: 42380           \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmtype-type-comp              sum: 20              \n",
            "[INFO,     24.856s, Stats]: size-zstd-mmtype-type-raw               sum: 42380           \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-decoding_case-comp       sum: 153             \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-decoding_case-raw        sum: 81743           \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-r1_split-comp            sum: 91              \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-r1_split-raw             sum: 368             \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-r2_split-comp            sum: 80              \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-r2_split-raw             sum: 368             \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-same_rec-comp            sum: 22592           \n",
            "[INFO,     24.856s, Stats]: size-zstd-pair-same_rec-raw             sum: 326788          \n",
            "[INFO,     24.856s, Stats]: size-zstd-pos-first-comp                sum: 444             \n",
            "[INFO,     24.856s, Stats]: size-zstd-pos-first-raw                 sum: 1307888         \n",
            "[INFO,     24.856s, Stats]: size-zstd-qv-steps_0-comp               sum: 4892091         \n",
            "[INFO,     24.856s, Stats]: size-zstd-qv-steps_0-raw                sum: 20348280        \n",
            "[INFO,     24.856s, Stats]: size-zstd-rcomp-rcomp-comp              sum: 26              \n",
            "[INFO,     24.856s, Stats]: size-zstd-rcomp-rcomp-raw               sum: 163440          \n",
            "[INFO,     24.856s, Stats]: size-zstd-rlen-rlen-comp                sum: 349             \n",
            "[INFO,     24.856s, Stats]: size-zstd-rlen-rlen-raw                 sum: 2615424         \n",
            "[INFO,     24.856s, Stats]: size-zstd-rname-comp                    sum: 691250          \n",
            "[INFO,     24.856s, Stats]: size-zstd-rname-raw                     sum: 5412076         \n",
            "[INFO,     24.856s, Stats]: size-zstd-rtype-rtype-comp              sum: 14069           \n",
            "[INFO,     24.856s, Stats]: size-zstd-rtype-rtype-raw               sum: 81767           \n",
            "[INFO,     24.856s, Stats]: size-zstd-total-comp                    sum: 5699769         \n",
            "[INFO,     24.856s, Stats]: size-zstd-total-raw                     sum: 31230238        \n",
            "[INFO,     24.856s, Stats]: size-zstd-ureads-ureads-comp            sum: 2983            \n",
            "[INFO,     24.856s, Stats]: size-zstd-ureads-ureads-raw             sum: 26167           \n",
            "[INFO,     24.860s, Stats]: time-fastq-export                       sum: 18.794000       \n",
            "[INFO,     24.860s, Stats]: time-namewriteout                       sum: 0.323000        \n",
            "[INFO,     24.860s, Stats]: time-qv-calq                            sum: 1.989000        \n",
            "[INFO,     24.860s, Stats]: time-spring-decoder                     sum: 1.259000        \n",
            "[INFO,     24.860s, Stats]: time-wallclock                          sum: 24.760000       \n",
            "[INFO,     24.860s, Stats]: time-zstd                               sum: 0.412000        \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def inspect_mgb_structure(host_dir=\".\", container_dir=\"/work\", mgb_filename=mgb_filename):\n",
        "    command = [\n",
        "        \"docker\", \"run\", \"--rm\",\n",
        "        \"-v\", f\"{host_dir}:{container_dir}\",\n",
        "        \"muefab/genie:latest\", \"run\",  # ✅ Add \"run\" subcommand here\n",
        "        \"-f\",\n",
        "        \"-i\", f\"{container_dir}/TestFiles/{mgb_filename}\",\n",
        "        \"-o\", f\"{container_dir}/TestFiles/{mgb_filename_no_mgb}.fastq\"\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(command))\n",
        "    result = subprocess.run(command, capture_output=True, text=True)\n",
        "    print(\"\\n--- STDOUT ---\\n\")\n",
        "    print(result.stdout)\n",
        "    if result.stderr:\n",
        "        print(\"\\n--- STDERR ---\\n\")\n",
        "        print(result.stderr)\n",
        "\n",
        "inspect_mgb_structure()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0baf4122-bd36-44d5-ac38-19b22c04e5ed",
      "metadata": {
        "id": "0baf4122-bd36-44d5-ac38-19b22c04e5ed"
      },
      "source": [
        "### 🧬 Step 6: Load FASTQ in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c1b2d7e2-678d-4b9f-a0c6-af471493c52b",
      "metadata": {
        "id": "c1b2d7e2-678d-4b9f-a0c6-af471493c52b",
        "outputId": "a21ae4fb-20dd-4890-8a2c-ede7f052457e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Total reads: 163440\n",
            "📏 Avg read length: 124.5 bp\n",
            "🎯 Avg quality score: 33.3\n"
          ]
        }
      ],
      "source": [
        "# Safer path for Windows (forward slashes or raw string)\n",
        "fastq_path = os.path.join(os.getcwd(), train_dir, f\"{mgb_filename_no_mgb}.fastq\")\n",
        "\n",
        "# Check if the file exists before parsing\n",
        "if not os.path.exists(fastq_path):\n",
        "    print(f\"❌ FASTQ file not found at: {fastq_path}\")\n",
        "else:\n",
        "    total_reads = 0\n",
        "    read_lengths = []\n",
        "    quality_scores = []\n",
        "\n",
        "    for record in SeqIO.parse(fastq_path, \"fastq\"):\n",
        "        total_reads += 1\n",
        "        read_lengths.append(len(record.seq))\n",
        "        quality_scores.extend(record.letter_annotations[\"phred_quality\"])\n",
        "\n",
        "    print(f\"🔍 Total reads: {total_reads}\")\n",
        "    print(f\"📏 Avg read length: {sum(read_lengths)/len(read_lengths):.1f} bp\")\n",
        "    print(f\"🎯 Avg quality score: {sum(quality_scores)/len(quality_scores):.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f4099a0c-9ce7-4c75-bfc6-85e2845b0646",
      "metadata": {
        "id": "f4099a0c-9ce7-4c75-bfc6-85e2845b0646",
        "outputId": "15b45547-6dd9-4521-ae82-047240c10eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 First 3 reads:\n",
            "\n",
            "🔹 ID: NB501656:452:H3WJWAFXY:2:11306:21744:4603\n",
            "🔹 SEQ: ATACGTAAGGACCGAGCGTTGTCCGGAATCATTGGGCGTAAAGGGTACGT...\n",
            "🔹 QUALITY: [36, 36, 36, 36, 36, 36, 36, 36, 36, 36]...\n",
            "\n",
            "🔹 ID: NB501656:452:H3WJWAFXY:2:11306:21744:4603\n",
            "🔹 SEQ: CCTGTTTGCTACCCACGCTTTCGTACCTCAGCGTCAGATAATGGCCAGAA...\n",
            "🔹 QUALITY: [36, 36, 36, 36, 36, 36, 36, 36, 36, 36]...\n",
            "\n",
            "🔹 ID: NB501656:452:H3WJWAFXY:1:11302:10166:2744\n",
            "🔹 SEQ: ATACGTAAGGACCGAGCGTTGTCCGGAATCATTGGGCGTAAAGGGTACGT...\n",
            "🔹 QUALITY: [36, 36, 36, 36, 36, 36, 36, 36, 36, 36]...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"🧪 First 3 reads:\\n\")\n",
        "for i, record in enumerate(SeqIO.parse(fastq_path, \"fastq\")):\n",
        "    print(f\"🔹 ID: {record.id}\")\n",
        "    print(f\"🔹 SEQ: {record.seq[:50]}...\")  # just preview first 50 bp\n",
        "    print(f\"🔹 QUALITY: {record.letter_annotations['phred_quality'][:10]}...\\n\")\n",
        "    if i >= 2:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7c1d61-e2d0-456a-bcbb-85c87a8cfd63",
      "metadata": {
        "id": "8c7c1d61-e2d0-456a-bcbb-85c87a8cfd63"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d2472b20-6b35-4b89-83da-b8df524e9557",
      "metadata": {
        "id": "d2472b20-6b35-4b89-83da-b8df524e9557"
      },
      "source": [
        "### 🧠 What MPEG-G Did (Plain English)\n",
        "\n",
        "Your `.mgb` file used the MPEG-G standard to store sequencing data efficiently. Here's what happened under the hood:\n",
        "\n",
        "- **Access Units (AUs)**: Think of these as independent blocks, like packets or video frames. Each AU can be decoded without needing the entire file.\n",
        "  \n",
        "- **Descriptor Streams**:\n",
        "  - `SEQUENCE`: These are the DNA letters (A, T, C, G...).\n",
        "  - `QUALITY`: Confidence for each base (used to assess sequencing accuracy).\n",
        "  - `READ_IDENTIFIER`: Name or ID of each read.\n",
        "\n",
        "- **Compression Techniques**:\n",
        "  - Redundancies in the reads and IDs were removed.\n",
        "  - Quality scores may have been quantized or entropy-coded.\n",
        "  - Optional reference-based compression could align reads to a known genome and store only differences.\n",
        "\n",
        "- **Output Format (`.fastq`)**:\n",
        "  - This format is standard in genomics: it includes the ID, DNA sequence, and quality scores for each read.\n",
        "\n",
        "MPEG-G is to genomics what `.mp4` is to video — a way to store large data efficiently without losing critical information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc553413-28af-492b-8739-2b8e442de41d",
      "metadata": {
        "id": "fc553413-28af-492b-8739-2b8e442de41d"
      },
      "source": [
        "### 🏃 Step 7: Run steps 4 & 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6076ad58-cd07-436e-bd21-bcf6cc3a6f02",
      "metadata": {
        "id": "6076ad58-cd07-436e-bd21-bcf6cc3a6f02"
      },
      "outputs": [],
      "source": [
        "# Set base directories\n",
        "notebook_dir = os.getcwd()\n",
        "container_dir = \"/data\"  # This is the container's path\n",
        "\n",
        "def decode_all_mgb_in_folder(folder_name):\n",
        "    host_dir = os.path.join(notebook_dir, folder_name)\n",
        "    for mgb_filename in os.listdir(host_dir):\n",
        "        if not mgb_filename.endswith(\".mgb\"):\n",
        "            continue\n",
        "\n",
        "        mgb_filename_no_ext = os.path.splitext(mgb_filename)[0]\n",
        "        print(f\"\\n🔄 Decoding: {mgb_filename}\")\n",
        "\n",
        "        command = [\n",
        "            \"docker\", \"run\", \"--rm\",\n",
        "            \"-v\", f\"{host_dir}:{container_dir}\",\n",
        "            \"muefab/genie:latest\", \"run\",\n",
        "            \"-f\",\n",
        "            \"-i\", f\"{container_dir}/{mgb_filename}\",\n",
        "            \"-o\", f\"{container_dir}/{mgb_filename_no_ext}.fastq\"\n",
        "        ]\n",
        "\n",
        "        print(\"Running:\", \" \".join(command))\n",
        "        result = subprocess.run(command, capture_output=True, text=True)\n",
        "\n",
        "        \"\"\"\n",
        "        Caution on printing out each line as this does take up memory.\n",
        "\n",
        "        print(\"\\n--- STDOUT ---\\n\")\n",
        "        print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"\\n--- STDERR ---\\n\")\n",
        "            print(result.stderr)#\n",
        "\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yj_2L-vwlekL",
      "metadata": {
        "id": "yj_2L-vwlekL"
      },
      "source": [
        "## 📢 N.B. Split your Train and Test files into smaller sub files\n",
        "\n",
        "Decoding the mgb files is time and compute intensive, we recommend splitting the train and test files into smaller bite size chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c325462b-a464-4605-b59f-ac6c228fe53c",
      "metadata": {
        "id": "c325462b-a464-4605-b59f-ac6c228fe53c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔄 Decoding: ID_AAFNOT.mgb\n",
            "Running: docker run --rm -v c:\\Users\\Reinhard\\Documents\\computer_vision_projects\\microbiome_classification_challenge\\TrainFiles:/data muefab/genie:latest run -f -i /data/ID_AAFNOT.mgb -o /data/ID_AAFNOT.fastq\n",
            "\n",
            "🔄 Decoding: ID_AAXPTO.mgb\n",
            "Running: docker run --rm -v c:\\Users\\Reinhard\\Documents\\computer_vision_projects\\microbiome_classification_challenge\\TrainFiles:/data muefab/genie:latest run -f -i /data/ID_AAXPTO.mgb -o /data/ID_AAXPTO.fastq\n",
            "\n",
            "🔄 Decoding: ID_AAYKAN.mgb\n",
            "Running: docker run --rm -v c:\\Users\\Reinhard\\Documents\\computer_vision_projects\\microbiome_classification_challenge\\TrainFiles:/data muefab/genie:latest run -f -i /data/ID_AAYKAN.mgb -o /data/ID_AAYKAN.fastq\n",
            "\n",
            "🔄 Decoding: ID_ABEZNS.mgb\n",
            "Running: docker run --rm -v c:\\Users\\Reinhard\\Documents\\computer_vision_projects\\microbiome_classification_challenge\\TrainFiles:/data muefab/genie:latest run -f -i /data/ID_ABEZNS.mgb -o /data/ID_ABEZNS.fastq\n"
          ]
        }
      ],
      "source": [
        "decode_all_mgb_in_folder(\"TrainFiles\")\n",
        "decode_all_mgb_in_folder(\"TestFiles\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mpegg_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
